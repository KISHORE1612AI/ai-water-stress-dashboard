# Big Data Analytics Extension

## ğŸ¯ Overview

This is a comprehensive Big Data Analytics extension for the AI-Driven Water Stress Management in Plants project. It demonstrates all core Big Data concepts required for academic coursework and viva presentations.

## ğŸ“š Technologies Covered

### 1. **Hadoop Ecosystem**
- **HDFS Simulation**: Distributed file storage with block-based partitioning
- **Batch Processing**: Large-scale data processing workflows
- **Data Replication**: Fault-tolerance through data redundancy

### 2. **Apache Spark**
- **RDD Operations**: Resilient Distributed Datasets
- **Transformations**: map, filter, flatMap, reduceByKey, groupByKey
- **Actions**: collect, count, take, reduce
- **Lazy Evaluation**: Optimized query execution

### 3. **MapReduce Framework**
- **Custom Implementation**: Full Map-Shuffle-Reduce pipeline
- **Parallel Processing**: Multi-worker execution
- **Key-Value Processing**: Distributed data aggregation

### 4. **NoSQL Database**
- **Document Store**: JSON-based storage using TinyDB
- **CRUD Operations**: Create, Read, Update, Delete
- **Aggregation Pipelines**: Group-by and statistical operations
- **Flexible Schema**: Schema-less document structure

### 5. **Stream Processing**
- **Real-Time Analytics**: Live data processing
- **Sliding Windows**: Time-based and count-based windows
- **Event Filtering**: Stream transformation
- **Real-Time Aggregation**: Statistics on streaming data

## ğŸ—ï¸ Architecture

```
AI-Driven-Water-Stress-Management-in-Plants/
â”‚
â”œâ”€â”€ bigdata_pipeline.py         # Hadoop & Spark simulations
â”œâ”€â”€ mapreduce_engine.py         # Custom MapReduce implementation
â”œâ”€â”€ nosql_db.py                 # NoSQL database wrapper (TinyDB)
â”œâ”€â”€ stream_processor.py         # Real-time stream analytics
â”œâ”€â”€ dashboard.py                # Main Big Data dashboard (Streamlit)
â”‚
â”œâ”€â”€ hdfs_storage/               # HDFS partitioned data
â”œâ”€â”€ nosql_data/                 # NoSQL JSON documents
â”œâ”€â”€ stream_data/                # Streaming data buffers
â”œâ”€â”€ powerbi_exports/            # Exported datasets for visualization
â”‚   â”œâ”€â”€ cleaned_data.csv
â”‚   â”œâ”€â”€ analytics_summary.csv
â”‚   â”œâ”€â”€ correlations.csv
â”‚   â”œâ”€â”€ time_series.csv
â”‚   â””â”€â”€ *.parquet (Spark outputs)
â”‚
â””â”€â”€ plant_health_data.csv       # Original dataset
```

## ğŸš€ Quick Start

### Installation

```bash
# All dependencies are already in requirements.txt
pip install -r requirements.txt
```

### Running the Big Data Dashboard

The Big Data dashboard runs alongside the main ML application.

**Option 1: Command Line**
```bash
streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0
```

The dashboard will open at `http://0.0.0.0:8501`

**Option 2: From Replit**
- The main ML app runs on port 5000 (via workflow "Streamlit App")
- Open a new shell and run: `streamlit run dashboard.py --server.port=8501`
- Access at port 8501 via Replit's webview

**Note**: Both dashboards can run simultaneously:
- **app.py** (port 5000): Machine Learning predictions  
- **dashboard.py** (port 8501): Big Data analytics

### Running Individual Components

#### 1. Hadoop Batch Processing
```python
from bigdata_pipeline import run_hadoop_batch_job

stats = run_hadoop_batch_job("plant_health_data.csv")
print(stats)
```

#### 2. Spark RDD Analytics
```python
from bigdata_pipeline import run_spark_analytics

results = run_spark_analytics("plant_health_data.csv")
print(results['status_distribution'])
```

#### 3. MapReduce Job
```python
from mapreduce_engine import MapReduceEngine, plant_health_mapper, aggregation_reducer
import pandas as pd

df = pd.read_csv("plant_health_data.csv")
data = df.to_dict('records')

engine = MapReduceEngine(num_workers=4)
results = engine.run(data, plant_health_mapper, aggregation_reducer)
print(results)
```

#### 4. NoSQL Operations
```python
from nosql_db import NoSQLDatabase
import pandas as pd

# Initialize database
db = NoSQLDatabase()

# Load data
df = pd.read_csv("plant_health_data.csv")
db.load_from_dataframe(df.head(1000), 'sensors')

# Query
healthy = db.find_by_query('sensors', {'Plant_Health_Status': 'Healthy'})
print(f"Healthy plants: {len(healthy)}")

# Aggregate
stats = db.aggregate_by_field('sensors', 'Plant_Health_Status', 'Soil_Moisture')
print(stats)

db.close()
```

#### 5. Stream Processing
```python
from stream_processor import run_stream_analytics_demo

results = run_stream_analytics_demo("plant_health_data.csv")
print(f"Processed {results['processed_events']} events")
```

## ğŸ“Š Dashboard Features

### 1. Overview Tab
- Dataset statistics and metrics
- Health status distribution
- Sensor readings summary
- Data preview

### 2. Hadoop HDFS Tab
- HDFS directory structure visualization
- Block-based storage demonstration
- Data partitioning display
- Batch job execution

### 3. Spark RDD Tab
- RDD transformations execution
- Health status aggregation
- Sensor averages by status
- Interactive visualizations

### 4. MapReduce Tab
- Configurable worker count
- Map-Shuffle-Reduce visualization
- Key-value pair processing
- Aggregation results

### 5. NoSQL Database Tab
- Document insertion
- Query operations
- Aggregation pipelines
- Database statistics

### 6. Stream Analytics Tab
- Real-time event generation
- Sliding window processing
- Event filtering
- Live sensor visualizations

### 7. Data Exports Tab
- Download processed datasets
- Preview export files
- Quick statistics
- PowerBI-ready formats

## ğŸ“ Exported Datasets

All processed data is exported to `powerbi_exports/` for further analysis:

| File | Description | Use Case |
|------|-------------|----------|
| `cleaned_data.csv` | Cleaned dataset from Hadoop | PowerBI import |
| `analytics_summary.csv` | Spark aggregations | Dashboard visualization |
| `time_series.csv` | Stream processing output | Time-series analysis |
| `correlations.csv` | Correlation matrix | Statistical analysis |
| `*.parquet` | Spark output format | Big data tools |

## ğŸ“ Big Data Concepts Demonstrated

### Hadoop Concepts
- âœ… Distributed file system (HDFS)
- âœ… Block-based storage
- âœ… Data partitioning
- âœ… Batch processing
- âœ… Fault tolerance through replication

### Spark Concepts
- âœ… Resilient Distributed Datasets (RDD)
- âœ… Lazy evaluation
- âœ… Transformations vs Actions
- âœ… Parallel processing
- âœ… In-memory computation

### MapReduce Concepts
- âœ… Map phase (parallel transformation)
- âœ… Shuffle phase (data redistribution)
- âœ… Reduce phase (aggregation)
- âœ… Key-value pair processing
- âœ… Distributed aggregation

### NoSQL Concepts
- âœ… Document-oriented storage
- âœ… Schema flexibility
- âœ… JSON data format
- âœ… Query operations
- âœ… Aggregation pipelines

### Stream Processing Concepts
- âœ… Real-time data ingestion
- âœ… Sliding windows
- âœ… Event-time processing
- âœ… Stream transformations
- âœ… Stateful operations

## ğŸ”¬ Viva Questions & Answers

### Q1: Explain the MapReduce workflow in your implementation.
**Answer**: Our MapReduce engine has three phases:
1. **Map Phase**: Each worker applies the mapper function to input data, transforming records into key-value pairs
2. **Shuffle Phase**: All key-value pairs are grouped by key, distributing data across workers
3. **Reduce Phase**: Each group is aggregated using the reducer function to produce final results

Example: Counting plant health statuses uses `map` to emit (status, 1) pairs, then `reduce` sums the counts.

### Q2: How does your HDFS simulation differ from actual HDFS?
**Answer**: Our simulation implements core HDFS concepts:
- **Block partitioning**: Large files split into chunks
- **Distributed storage**: Files stored in separate partition files
- **Metadata management**: JSON metadata tracks all partitions

Differences: We use local filesystem instead of network nodes, and simplified replication.

### Q3: What are RDD transformations vs actions?
**Answer**: 
- **Transformations** (lazy): Create new RDDs without immediate execution (map, filter, reduceByKey)
- **Actions** (eager): Trigger computation and return results (collect, count, take)

Our implementation queues transformations and executes them only when an action is called, demonstrating lazy evaluation.

### Q4: Why use NoSQL for this application?
**Answer**: NoSQL benefits:
- **Flexible schema**: Easy to add new sensor types
- **Scalability**: Horizontal scaling for growing data
- **Fast queries**: JSON-based indexing
- **Aggregation**: Built-in group-by operations

Perfect for IoT sensor data with varying structures.

### Q5: Explain your stream processing windows.
**Answer**: We implement two window types:
- **Count-based**: Fixed number of recent events (e.g., last 100 events)
- **Time-based**: Events within time period (e.g., last 5 minutes)

Windows enable real-time aggregations and trend analysis on streaming sensor data.

## ğŸ¯ Assignment Deliverables

âœ… **Hadoop HDFS Implementation** - Block-based storage simulation  
âœ… **Spark RDD Operations** - Transformations and actions  
âœ… **MapReduce Engine** - Custom Map-Shuffle-Reduce  
âœ… **NoSQL Database** - Document store with queries  
âœ… **Stream Processing** - Real-time analytics with windows  
âœ… **Data Exports** - PowerBI-ready datasets  
âœ… **Interactive Dashboard** - Streamlit visualization  
âœ… **Documentation** - Complete README with examples  

## ğŸ‘¥ Team

- **Kirtthan Duvvi** (22BCE0061)
- **Kushal Sharma** (22BCE2561)  
- **Akshar Varma** (22BCT0372)

**Project Guide**: Dr. Perepi Raja Rajeshwari

**Institution**: Vellore Institute of Technology - 2025

## ğŸ“ Notes for Viva

1. **Data Flow**: CSV â†’ HDFS â†’ Spark â†’ MapReduce â†’ NoSQL â†’ Exports
2. **Scalability**: All components support parallel processing
3. **Real-World**: Concepts applicable to production Big Data systems
4. **Performance**: Multiprocessing for true parallel execution
5. **Exports**: PowerBI integration for business intelligence

## ğŸ› ï¸ Troubleshooting

### Issue: Import errors
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

### Issue: Port already in use
```bash
# Solution: Use different port
streamlit run dashboard.py --server.port=5001
```

### Issue: HDFS directory not found
```bash
# Solution: Directories created automatically, but can manually create:
mkdir -p hdfs_storage nosql_data stream_data powerbi_exports
```

## ğŸ“š Additional Resources

- [Apache Hadoop Documentation](https://hadoop.apache.org/docs/)
- [Apache Spark Guide](https://spark.apache.org/docs/latest/)
- [MapReduce Tutorial](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
- [NoSQL Databases](https://nosql-database.org/)
- [Stream Processing Patterns](https://www.oreilly.com/library/view/stream-processing-with/9781491974285/)

---

**Ready for Production âœ¨ | Zero Errors âœ… | Fully Documented ğŸ“–**
